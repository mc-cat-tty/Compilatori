# Introduzione
## Memory wall
Moore's law vs DRMA growth: le performance delle DRAM crescono estremamente lentamente rispetto alle performance della CPU. Il cap aumenta nel tempo.

Nascono le cache per colmare il gap tra velocità di CPU e DRAM. Nel 1980 non esistevano cache, nel 1995 vengono introdotte le cache di doppio livello.

Le cache cercano di mantenere il numero di IPC alto. Idealmente 1:
- vero per istruzioni che usano ALU
- falso appena incontriamo un accesso in memoria (load/store)
Le cache aiutano a mantenere il IPC alto.

## DRAM
Problema: la DRAM è una memoria off-chip. Si trova distante dal SoC. Inoltre, è dinamica, ovvero i capacitori che compongono le rows (righe di bit) devono essere refreshati per mantenere la carica e, di conseguenza, l'informazione che trasportano.

>**Row**: unità minima organizzativa, rettangolare della DRAM. L'accesso viene fatto sull'intera riga di cache.

Le frequenze di refresh aumentano nel tempo:
- DRAM - Double Data Rate: refresh su entrambi i fronti
- QDR - Quad Data Rate: separazione dei canali di input e output

#Vedi metriche di performance della memoria: banda e latenza.

La banda può essere aumentata parallelizzando la lettura: trasferimento di bit della stessa riga parallelamente. Viene chiamata **burst mode**.

## SRAM
>Static RAM: memoria composta da transistor (non capacitori come sopra). Ogni cella è composta da 6 transistor MOS, che formano un flip-flop.

Vantaggi:
- high density
- fast access time
- high cost

La SRAM è fisicamente distribuita. Eg. L3$ corre su tutto il chip, circondando i vari core.

#Ricorda la distinzione tra *instruction* e *data* cache. Permette la realizzazione di una architettura di Harvard.

## Gerarchie di memoria
>Blocco o linea: unità della copia. Può essere formata da più word.

- cache miss: il dato non si trova sulla cache di livello più alto, fallisce il fetch dalla cache, paga la penalità di caricare una nuova riga di cache
- cache hit: il dato si trova in cache

## Località
>**Località spaziale**: i programmi spendono la maggior parte del tempo nei loop. I loop tipicamente accedono a strutture dati contigue in memoria.
>**Località temporale**: fatto un accesso ad un blocco, probabilmente verrà riacceduto in futuro.

#Vedi *Program reconstructing for virtual memory*. IBM System Journal.

## Cache misses
Esistono 3 tipi di cache nei processori single-core:
- **cold cache** (compulsory): succede a sistema freddo
- **conflict**: succede nelle cache direct mapped, quando due indirizzi vorrebbero essere cachati allo stesso indice (index bits).
- **capacity**: Le cache completamente associative non usano index bits. Possono comunque finire il numero di linee di cache libere. Anche questo è un tipo di miss.

## Cache schemes
Nella realtà si usano cache **set associative**: cache che mischiano entrambe le strategie. La componente direct mapped fornisce il set di linee individuate dallo stesso indirizzo (in particolare, dagli index bits). Eg. una cache 2-way set associative usa gli index bits per recuperare un set di 2 linee, confrontando poi (parte associativa) l'effettivo indirizzo in memoria.

Più lo schema è associativo, meglio utilizzo lo spazio. Ma rendo lo schema più costoso, dato dal numero di comparatori.

## Block sizes
Linee di cache di grandi dimensioni, permettono di sfruttare meglio il principio di località spaziale. Ma riducono il numero di linee disponibili, alzando la pollution (eviction dovuta a un conflitto).
## Cache snooping
>Schema nel quale uno snooper (o coherency controller) monitora gli accessi in memoria la fine di mantenere coerenti le cache di un sistema distribuito..

# Memory optimizations
## Riuso e località
>**Data reuse**: dati vicini sono usati più volte. Sfrutta località temporale.

Pensa ad esempio alle moltiplicazioni matriciali, in cui una riga i viene usata più volte per calcolare gli elementi (i, j), (i, j+1), ...
## Classi di variabili
- **scalari**: eg. variabili intere
- **strutture e puntatori**
- **array**: lasciano più margine di manovra
### Scalari
Tre tipi: locali, globali, argomenti

Le variabili scalari sono memorizzate in registri. Nell'intermedio ottenuti con mem2reg, sotto l'assunzione che esista un numero illimitato di registri (se non conosco la struttura del **register file**). Il suo duale, reg2mem viene usato per riportare la gestione della memoria nell'intermedio.
### Strutture e puntatori
>Le struct sono differenti dalle strutture viste finora in quanto lasciano al programmatore la possibilità di personalizzare il tipo di dato.

Problema: campi che cadono su più row, mi costringono a carica più row, dove non è strettamente necessario.

Soluzioni:
- **layout alignment**
- **padding**: se non è possibile con semplici riordini, aggiungo dummy bytes per spingere l'allineamento alle parole di macchina

```c
struct {
	int count;
	double velocity;
	double inertia;
	struct node neigh[N];
}

for (int i=0; i<N; i++) {
	total_count += neigh[i].count;
}
```
Problema: ho un accesso ripetuto a linee diverse, sulle quali accedo al primo campo.
Soluzione: il compilatore potrebbe spezzare la struttura dati, smembrandola, per fare stare tutti i count sulla stessa linea.

Considera due scenari:
- come ottimizzare una struttura per l'accesso ai campi?
- come ottimizzare una struttura per l'accesso allo stesso campo iterativamente?

## Nested loops
```c
for i = 0 to N-1
	for j = 0 to N-1
		A[i][j] = B[j][i]
```
Idea: sto trasponendo la matrice A

#Nota: differenza tra analisi statica e profiling
### Iteration space
>Definisce l'ordine di visita delle celle della matrice.

#Attenzione **iteration space** differisce dal **data space**
#Visualizza come un reticolo dove ogni punto 2D rappresenta una coppia (i, j) e i vettori rappresentano l'ordine di accesso

Il pattern di accesso influenza il pattern delle miss. Esempio: cache con 8 linee da 4 word, dove ogni word è una coordinata del reticolo.
- In A, ogni 4 punti viene compiuta una compulsory miss entro le prime 8, poi per le successive conflict miss
- In B, ogni punto è una miss, inizialmente per cold cache, poi per compulsory. L'accesso verticale è poco **cache-friendly**.

Come ottimizzare?
- quando accade una miss? **locality analysis**. Possiamo riordinare le istruzioni per ottimizzare l'uso della località spaziale?
- quali sono le dipendenze? **dependence analysis**. Permette di parallelizzare, senza alterare il risultato.

#Vedi **approximate analysis**: calcolo approssimativo, ma tollerabile dall'applicazione, ordinando le operazioni in modo che siano più veloci da eseguire rispetto alla versione originale. Idea contrastante rispetto al calcolo scientifico. Viene usato in motori di ricerca, algoritmi di machine learning, etc. L'algoritmo di Monte Carlo ne è un esempio.
### Loop interchange
>Chiamata anche **loop permutation**, **loop permutation**, **loop skewing**, permette di riordinare l'ordine (**column-major** vs **row-major**). Ha diversi benefici:
> - riorganizzare i loop per abilitare il giusto livello di parallelismo: **thread** (grana grossa: un major ad ogni thread) vs GPU-parallel/**unità vettoriale** (grana fine: un elemento per core)
> - sblocca altre ottimizzazioni

#Nota che la grana grossa conviene nei casi in cui l'overhead cresca con il numero di worker paralleli.

### Tiling: cache blocking in 2D
>Avviene in situazioni in cui i blocchi della struttura dati sono così grandi che non stanno in cache. Eg. la riga di una matrice non sta in cache

La moltiplicazione matriciale è un esempio perfetto:
```c
for i = 0 to N-1
	for j = 0 to N-1
		for k = 0 to N - 1
			res[i][j] += a[i][k] * b[k][j]
```

Dove:
- `a[i][k]` accede un'intera riga più volte, per ogni colonna di b
- `b[k][j]` ha scarsa località (cache-unfriendly)

Idea: **riuso esplicito** di parti delle righe di a (tiles), assumendo che una riga intera non stia in cache. Mantengo in cache una porzione di matrice finché non ho esaurito tutte le operazioni su quella tile.

#Vedi NVIDIA gpu programming courses, software-managed buffers

Si implementa aggiungendo dei livelli di looping.
## Loop fusion
>Scenario in cui loop sequenziali accedono con lo stesso pattern (**iteration space** uguale: **iteration stride** and **iteration space size** uguali), esibiscono **località temporale**, c'è **riuso**; ma la dimensione delle cache non lo permette.
>Idea: fondere i due loop sotto la stessa struttura di controllo

Problema: la **footprint** della matrice dovrebbe essere grande quanto la cache, assunzione improbabile.

```c
for (i=0; i<N; i++) {
	for (j=0; i<N; i++) {
		a[i][j] = 1/b[i][j] * c[i][j]
	}
}

for (i=0; i<N; i++) {
	for (j=0; i<N; i++) {
		d[i][j] = a[i][j] + c[i][j]
	}
}
```

Focalizziamoci sulla matrice `a`. Se la dimensione della cache è la metà della cache size, ho tutte miss (primo 1/4 compulsory e 3/4 conflict).

Il programma esibisce località temporale ma non la sfrutta. Passa tempo prima che le celle di cache vengano riusate. La loop fusion fornisce la garanzia che la cella a cui si accede sia già in cache.

Preambolo per l'ottimizzazione: footprint di accesso (`end-start/stride`) maggiore della dimensione di cache.

#Vedi loop fusion e fission
## Software prefetching
>Tecnica per ammortizzare il costo di accesso. Chiamata anche **latency hiding**.
>Tipicamente gestita in hardware dalle linee di cache.
>Così importante che può essere realizzata anche in-software.

Idea: overlap memory accesses with computation and other accesses. Porto avanti una miss preventiva per poi fare solo hit.
Divisa in **data** e **instruction** prefetching. Le istruzioni hanno tipicamente un pattern di accesso più prevedibile.

Problemi:
- dove mettere l'istruzione di prefetching? **prefetch distance** = tempo di fetch. Eg. 100 cicli, dove in mezzo metto istruzioni, secondo il loro IPC
- posso anticipare tutto? no, **interferenza/cache trashing**

Queste ottimizzazioni si suddividono in due fasi: analisi e scheduling.

Vedi: loop peeling e loop unrolling.

Software pipelining example: $iteration ahead = ceil(l/s)$ con $l$ latenza di memoria e $s$ shortest path through loop body
```c
for (int i=0; i<100; i++) {
	a[i] = 0
}
```
Diventa, dopo un peeling e prefetching:
```c
// Prolog
for (int i=0; i<5; i++) {
	prefetch(&a[i])
}

// wait until the a[0] has been fetched

// Steady state
for (int i=0; i<95; i++) {
	prefetch(&a[i+5])
	a[i] = 0
}

// Epilog
for (int i=95; i<100; i++) {
	a[i] = 0;
}
```

# Loop Fusion Assignment
>Obiettivo: avere la garanzia che il secondo accesso ad un'area di memoria sarà una hit.

Condizioni necessarie alla fusion:
1. preambolo: stessa dimensione dello spazio di iterazione, stesso stride, adiacenza, etc.
2. analisi di profitto: se la cache è grande a sufficienza, non è necessaria la loop fusion

## Condizioni
Condizioni per la fusion di due loop $L_i$ e $L_j$:
1. **Sibling loops**: $L_i$ e $L_j$ devono essere adiacenti. Non possono esserci istruzioni tra essi
2. **Same iteration space**: $L_i$ e $L_j$ devono iterare lo stesso numero di volte. Altrimenti la fusione non sarebbe equivalente
3. **Control flow equivalence**: se esegue il primo, deve sempre eseguire anche il secondo
4. **Non** ci possono essere *negative distance dependencies* tra i due

## Struttura del passo
Obiettivo: voglio una vista globale su tutti i loop (livello di funzione), ma anche di poter iterare sui loop handles.

```C++
PreservedAnalyses LoopFusionPass::run(Function &F, FunctionAnalysisManager &AM) {
	LoopInfo &LI = AM.getResult<LoopAnalysis>(F);  // AM contiene il risultato di analisi precedenti
	for (auto *L : *LI) {}
}
```

#Idea: worklist per esplorare loop annidati #Completa 

## Adiacenza
Idea: due loop sono adiacenti se non esistono istruzioni in mezzo ai due loop. Aka non ci sono basic block in mezzo.

Tipi di loop (https://llvm.org/docs/LoopTerminology.html):
- **non guarded loop**: di cui abbiamo parlato finora. Exit block del primo coincide con preheader del secondo.
- **guarded loop**: esiste un blocco di guardia prima del loop, quindi non è vera la condizione sopra

## Control flow equivalence
Ci servono le informazioni di dominanza e post-dominanza. L0 domina L1 e L1 post-domina L0.
>Post-dominance: A postdomina B se in tutti i percorsi da B all'uscita è presente A.+

## Scalar Evolution Analysis
>Analisi dell'espressione con cui una variabile di loop evolve, al fine di ricavare il **trip count**.

#Vedi: induction variable elimination. Una sorta di DCE.

Obiettivo finale: trasformare un loop in una singola espressione, o comunque rimuovere la struttura iterativa.

Per accedere all'analisi di scalar evolution:
```C++
#include "llvn/Analysis/ScalarEvolution.h"

// ...

ScalarEvolution &SE = AM.getResult<ScalarEvolutionAnalysis>(F);
```

## Data Dependence Analysis
Voglio evitare la fusione di loop del tipo:
```c
for (int i=0; i<N; i++) {
	a[i] = ...;
}

for (int i=0; j<N; i++) {
	b[i] = a[i+3];
}
```

Ottenibili dalle API di LLVM:
```C++
DependenceInfo &DI = AM.getResult<DependenceAnalysis>(F);
auto dep = DI.depends(&instr0, &instr1, true);
```

Serve `#include "llvm/Analysis/DependenceAnalysis.h"`