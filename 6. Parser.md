
# Parse tree
O alberi di derivazione
È un albero radicato ed etichettato:
- i nodi interni sono etichettati con simboli non terminali
- le foglie sono etichettate con simboli terminali o $\epsilon$
- la relazione padre-figlio è dettata dalle produzioni (nel nodo la parte destra, nelle foglie la parte sinistra)
- nella radice si trova l'assioma

Un albero sintattico descrive una derivazione? sì e no. Ad un parse tree possono corrispondere più derivazioni, ma questo non determinismo è marginale rispetto alla struttura dell'albero.

## Derivazioni canoniche
Derivazioni canoniche destre e sinistre.
- destra: sviluppa sempre il non terminale più a destra
- sinistra: sviluppa sempre il non terminale più a sinistra

Qual'è meglio? dipende dal tipo di parsing. Per il top down è meglio sinistre, per il bottom up è meglio derivazioni destre.

Fissata una derivazione canonica esiste un cammino unico sull'albero.

## Grammatiche ambigue
Si chiamano **ambigue** le grammatiche per cui anche se fissi una derivazione canonica per una stessa stringa possono essere generati parse tree differenti.

#x019

>Il parsing è il passaggio da una struttura lineare a una struttura bidimensionale

#Vedi if aperti (senza else) e chiusi. L'else viene fatto matchare con l'if più vicino.

Condizionale aperto e chiuso. Esiste una grammatica senza interpretazioni ambigue.
#Completa 

## Grammatiche cicliche
Se sono presenti derivazioni del tipo $A \Rightarrow A$
I parser usano algoritmi deterministici. Seguita una strada di produzioni seguiranno sempre quella. Si possono piantare in cicli.

I parser usano un *lookahead* -> sguardo in avanti di N simboli

## Grammatiche con prefissi comuni
Con due o più produzioni relative allo stesso nonterminale
$$S \to A\alpha_1$$ $$S \to A\alpha_2$$

Algo che risolve il problema:
- si numerano i terminali
- non può avvenire che una produzione abbia a destra un non terminale di grado più basso del non terminale di sinistra. Devono essere sempre piccolo -> grande

Viene snaturata leggermente la grammatica

#x020 Verrà mai fuori la derivazione sbagliata? No, viene forzato l'uso delle parentesi
#x021 usa sempre l'ordine degli operatori corretto

#Ricorda di sommare sempre dai valori più piccoli a quelli più grandi

Associatività:
- per, più e diviso a sinistra
- sottrazione ed esponenziale associativi a destra

Grammatica che parsa correttamente tutte le espressioni aritmetiche:
#completa con slide *Precedenza degli operatori*

#Esercizio grammatica libera per il linguaggio binario contenente tutte e sole le stringhe con più zeri che uni. O in generale tutte e sole le stringhe con un diverso numero di 1 e 0.

# Parser a discesa ricorsiva
#Completa fino a slide 11
>Un parser a discesa ricorsiva...
>Può essere implementato come una collezione di procedure, una per ogni simbolo non terminale.

## Parser non deterministico
Pseudocode per **parser non deterministico**:
```
scegli opportunamente uan produzione A -> X1X2...Xk : Xj appartiene a V
for j = 1 .. k (per ogni simbolo)
	se Xj è non terinale
		chiama Xj()
	else
		x = next_token()
		if Xj != x then
			error()
```

#Nota la conoscenza delle produzioni è cablata nello pseudocodice, non è quindi generalizzabile

#Nota l'errore include anche EOF, ovvero sviluppo na frase che che termina prima del dovuto (dall'input)

## Parser deterministico con backtracking
Supposta `Xj` puntatore globale
```
Carico input in memoria
saveInputPointer()
for all production A -> X1X2...Xk : Xj in V
	fail = False
	for j in i..k
		if Xj non-terminal and Xj() // procedura Xj va avanti e se ok torna true
			continue
		if Xj terminal
			x = next_token()
		if Xj == x
			continue
		restoreInputPointer() // torno indietro e provo a matchare altro
		fail = True
		break;
	
	if not fail
		return True
return False
```

L'assioma può essere sviluppato su tutto l'input (successo) o solo su un suo prefisso (fallimento):
```
if eof()
	return True
else
	return  False
```

#Nota nell'albero di esempio si parte dalla foglia più a sinistra e poi si tenta di matchare verso le foglie di destra fino a quando non si ha successo con `b`
#Completa Aggiungi immagine
## Rimozione dei cicli
#completa
È necessario eliminare cicli diretti e indiretti, ovvero le ricorsioni sinistre.
L'idea è quella di numerare le produzioni.
#completa sotto con condizioni
In base alla relazione tra gli indici di destra e sinistra ogni produzione può essere:
- ciclo diretto
- forward production
- backward production
#Completa con algoritmo diretto
### Cicli diretti
$$E \to E +E | E*E | (E) | number$$

- $\alpha_1 = E+E$
- $\alpha_2 = E*E$
- $\beta_1 = (E)$
- $\beta_2 = number$

Le produzioni vegnono sostituite:
- $E \to (E)E' | number E'$
- $E' \to +EE'|*EE'|\epsilon$

### Eliminazione delle backward production
1. Per $A_1$ non può esistere una backward production
2. Può esserci un ciclo diretto, eliminabile come sopra
3. Ipotesi induttiva: per un dato indice $i \ge 2$ per i primi $i-1$ non terminali non ci sono backward production
4. Condsideriamo $A_i$ e supponiamo che una delle sue produzioni siano di tipo backward: $A_i \to A_j\alpha |..., j < i$
5. $A_j$ può essere sostituita con una ad una tutte le parti delle sue produzioni
6. #Completa 

## Codice di esempio
- I simboli (la enum che contiene la rappresentazione dei simboli) sono definiti dal parser e rispettati dal lexer
- Il file .lex importa la definizione
- La struttura è `{integer} {value = std::stoi(yytext); return tok_num;}` oppure senza token value `"+" {return tok_plus;}`
- Il lexer va compilato in un file oggetto, da linkare successivamente al parser
- Inclusione di `FlexLexer.h` necessaria per il linking
# Kaleidoscope
## Paradigma funzionale
>Paradigma in cui non esiste il concetto di memoria (dal POV modellistico), ma solo quello di ambiente. Gli identificatori sono direttamente legati agli oggetti. I programmi sono costituiti solo da **espressioni** e **chiamate di funzioni**.

I linguaggi funzionali sono turing-completi -> si possono fare le stesse identiche cose

Dove memorizzo il risultato di una chiamata a funzione? ad un'altra chiamata a funzione, oppure lo uso in un'espressione.
### Esempio
Si prenda il seguente statement in C:
```c
int a = 1;
```

Questa associazione si ottiene mediante due funzioni:
- Ambiente `a -> [ mem ]` mapping identificatore -> locazione di memoria
- Store `[ mem ] -> 1` mapping locazione di memoria -> valore

Nei linguaggi funzionali esiste solo l'ambiente, l'environment; 

Corollario:
- Assegnamento (`a = 2`) modifica lo store
- Dichiarazione modifica l'ambiente

>Se in Python esistessero solo oggetti immutable, esso sarebbe un linguaggio funzionale. Per questi oggetti esiste un correlazione diretta tra identificatore ed oggetto/valore.

Nota:
```Python
a = 1
a = (1, 2)
```

Cambio completamente dove punta la reference di `a`

**PRO**
Il linguaggi funzionali riducono notevolmente gli errori dovuti a store e side-effects, che costituiscono il 95% dei problemi dei linguaggi imperativi.

**CONTRO**
Comportano un altissimo spreco di risorse.

Snippet in Kaleidoscope:
```kal
def fib(x)
	if x <= 1
		then 1
	else
		fib(x-1) * fib(x-2)
end;
```

#Nota che non esistono assegnamenti, solo espressioni. Anche l'if è un'espressione. Non esiste il `return` perché ha un sapore imperativo.

## Grammatica di Kaleidoscope
Con sintassi di Backus-Naur:
```
<def> ::= def <proto><expr> // definizione di funzione
<external> ::= external <proto> // signature external, da linkare
<top> ::= <def> | <external> | <exp> | eps
<program> ::= <top> ; <program> | eps
```

#Nota mi aspetto che una sequenza di punti e virgola sia accettata dal linguaggio

```
...
```

# Parser predittivi
## Parser top-down a scelta diretta
>Una soluzione migliore rispetto ad usare un'esplorazione esaustiva delle derivazioni -> uso un lookahead per scegliere la prossima produzione da utilizzare

#Completa tutto il capitolo
## Parser LL(1)
>Un parser predittivo può essere realizzato facilmente con grammatiche LL(1)

Sono nel generico caso in cui: 
Una grammatica LL(1), ricordando che usiamo derivazioni canoniche sinistre, 

$$
	A \to \gamma | \delta
$$
È più facile dire quando la grammatica NON è LL(1): 
- una produzione porta ad un'ambiguità
- RHS è così composta:
	- produzione nulla o nullificabile
	- produzione che porta *a* a seguire A

#Vedi esempi

### First e Follow
>$FIRST(\alpha)$ è l'insieme dei simboli non terminali, epsilon incluso, che possono precedere $\alpha$

Come calcolare FIRST di X (definizione ricorsiva)?
- se X terminale > $FIRST(X) = \{X\}$
- se X è non terminale -> init $FIRST(X) = \{\}$.
	- Se esiste una produzione generica $X \to X_1X_2...X_n$ e, dato un generico j
#Completa 

L'idea è quella che ricosrivamente si prenda il primo simbolo del RHS di una produzione, se nullificabile si scali al successivo, se terminale si aggiunge all'insieme.

>$FOLLOW(A)$ sono tutti i simboli $a$ che possono seguire $A$ in una forma di frase

Il calcolo del FOLLOW per un generico non terminale A può essere svolto come: se esiste la produzione $B \to A \alpha B$ allora $FOLLOW(\alpha)$ 

#Attenzione Se esiste la produzione $B \to \alpha A \beta$ Nel $FOLLOW(A)$ ci sta anche tutto il $FOLLOW(B)$


#Nota si calcolano sempre prima i first, per riutilizzare gli insiemi costruiti nei follow
#Nota nel follow non si considerano mai gli epsilon, ma sempre il simbolo di EOF (`$`)
#Attenzione a situazioni cicliche in cui il follow di un non terminale include il follow di un'altro

## Definizione formale di grammatiche LL(1)
Una grammatica è LL(1) se, qualora esistano due produzioni $A \to \alpha$ e $A \to \beta$, risulta che i due FIRST sono disgiunti: $$FIRST(\alpha) \cap FIRST(\beta) = \{\}$$

Possiamo costruire una tabella di parsing: le righe sono indicizzate da non terminali, le colonne da terminali. In questo modo con grammatiche LL(k) all'intersezione può trovare al più k produzioni da utilizzare.

#Nota il payoff è molto elevato: basta uno stack e la tabella di parsing

Le keyword giocano un ruolo fondamentale nel rendere la grammatica LL(1).

# Tabella di parsing
Tante righe quanto i simboli non terminali, tante colonne quanti sono i terminali + 1, ovvero l'eof.

Si guardano i first se non nullificabile, anche i follo altrimenti.
Una grammatica è LL(1) se all'intersezione tra ogni riga e ogni colonna esiste una sola produzione. Le celle vuote corrispondono ad una situazione di non-riconoscimento della stringa.

Il programma di controllo diventa uno stack + un input sequenziale/unidimensionale (no necessità di bufferizzare). 
#Ricorda di pushare il RHS della produzione in ordine inverso. Eg: *TE'* viene inserito come *E'*, poi *T*

Condizioni di rifiuto:
- pop di un simbolo terminali che non corrisponde con l'input
- risolvendo nella tabella per input, elemento poppato si trova una cella vuota

Il tempo di parsing è cubico nel numero di token.
# Costruzione del codice intermedio
Ad ogni reduce verrà triggerato una funzione di callback per la costruzione del codice intermedio.

Su Bison il paradigma sarà proprio questo.