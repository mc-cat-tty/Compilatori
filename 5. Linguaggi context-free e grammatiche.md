# Limiti dei linguaggi regolari

## Linguaggio delle parentesi bilanciate
In ogni punto della stringa devono esserci più parentesi aperte che parentesi chiuse, al più si possono uguagliare. Alla fine della stringa devono essere bilanciate.

Parentesi accoppiate servono per definire blocchi di codice.

Queste caratteristiche non sono esprimibili attraverso linguaggi regolari. -> gli automi, che non hanno memoria, o meglio non possono avere un numero variabile di stati, non possono contare le parentesi

#Approfondimento Metodo di Horner: modo più efficiente per calcolare un polinomio in un punto.

#Esercizio un linguaggio consente operandi e l'operatore *+*, il cui numero di parentesi aperte (due token aggiuntivi: parentesi aperte e chiuse) e non ancora chiuse sia al più due. Si può riconoscere con un automa deterministico.

>Le regex non sono lo strumento adatto per descrivere la sintassi, vanno bene giusto per il lessico

# Linguaggi context-free
Sono lo strumento giusto per descrivere la sintassi dei linguaggi di programmazione; ma non tutte le caratteristiche sono definite mediante linguaggi context-free: ad esempio verificare che una variabile sia stata definita prima dell'utilizzo, oppure che i parametri attuali e formali siano coerenti. Si chiamano strutture **context-dependent** (verificate attraverso l'analisi semantica)

Le grammatiche context-free descrivono pezzo per pezzo il linguaggio di programmazione.

# Le grammatiche
>Una grammatica è un formalismo **generativo** (a differenza delle espressioni regolari, mediante una grammatica è possibile generare tutte e sole le frasi che appartengono al linguaggio).

Indicate con il simbolo $G$

Come può agire un parser?
1. **parsing top down**: parte da un assioma iniziale e cerca di arrivare alla frase
3. **parsing bottom up**: parte dalla frase e cerca di tornare all'assioma iniziale

## Definizione formale
È una quadrupla di elementi: $G = (N, T, P, S)$
- $N$ insieme di simboli detti *simboli non terminali* -> simboli che non si trovano nelle frasi del linguaggio. Useremo le lettere maiuscole. Giocano un ruolo intermedio/strutturale.
- $T$ insieme di *simboli terminali*  non sovrapposto con $N$. Li indicheremo con lettere minuscole
- $P$ insieme di *produzioni* o regole di riscrittura: scritture nella forma $X \rightarrow Y$. Tutte le $X$ possono essere sostituite con $Y$
- $S$ simbolo iniziale o *assioma*

## Produzioni e derivazioni
Produzioni:
- lineari destre
- lineare sinistre
#completa

>Una grammatica genera il linguaggio costituito dalle sequenze di simboli terminali derivabili dall'assioma iniziale $S$

Una **derivazione** è una sequenza di produzioni o regole di riscrittura che genera a partire dall'assioma un simbolo terminale.
#Nota per indicare la produzione si usa $\rightarrow$, per indicare la derivazione si usa $\Rightarrow$. Per eliminare le ambiguità.
#completa con esempio da S a *ab*

#Prova a dimostrare che da S si può arrivare a *aab*

Una derivazione può dare origine ad un albero, *senza biunivocità*

## Descrizione succinta convenzionale di una grammatica
La parte fondamentale della grammatica è l'insieme delle produzioni. Si applicano convenzioni per la descrizione simili a quelle per gli automi.
- $S$ viene ricavato dalla prima produzione. Simbolo a destra della prima produzione.
- Lista di produzioni nella forma: `SIMBOLO NON TERMINALE -> SIMBOLO TERMINALE o NON TERMINALE`. Il contrario (simbolo terminale sulla sinistra e non terminale sulla destra) non sarebbe ragionevole perché non saprei come sbarazzarmi del simbolo non terminale a sinistra.
Per economizzare la descrizione si introducono metasimboli come il pipe: qesto simbolo raggruppa più RHSs sulla stessa testa LHS.

## Forma di frase
> Una **forma di frase** è una struttura che si può estrarre dai passaggi intermedi della derivazione. Le forme di frase contengono simboli non terminali.

> Le **frasi**, al contrario, contengono solo simboli terminali.

## Ambiguità
Le **derivazioni alternative** non sono buone -> la struttura dell'albero accenna già, oltre alla sintassi, la semantica della frase.

Si prenda ad esempio la seguente frase da parsare con la regola `E -> E x E | E + E | (E) | n`: `n + n x (n+n)`. Esistono due parse tree distinti a causa delle due derivazioni distinte:
- $E \Rightarrow E+E \Rightarrow E + E \times E \Rightarrow E + E \times (E) \Rightarrow E + E \times (E + E) \xRightarrow{*} n + n \times (n+n)$
- $E \Rightarrow E \times E \Rightarrow E + E \times E \Rightarrow E + E \times (E) \Rightarrow E + E \times (E + E) \xRightarrow{*} n + n \times (n+n)$

L'albero corretto è il primo, in quanto nel nodo radice si trova l'ultimo operando da applicare.

#Esercizio: dimostrare che una grammatica genera un linguaggio

## Grammatiche context-free vs context-dependent
Perché si chiamano linguaggi liberi (generati da grammatiche senza contesto)? mentre si applica una derivazione, si trova una forma di frase (intermedia). Compare un simbolo non terminale. Si può applicare qualsiasi produzione tra quelle presenti per sviluppare quel simbolo, indipendentemente da quello che gli sta intorno.

Nelle grammatiche con contesto due *carabinieri* mi restringono i valori ammissibili intorno al simbolo non terminale che voglio sviluppare:
$$\beta A \gamma \rightarrow \beta \alpha \gamma$$
$\beta$ e $\gamma$ sono il contesto

Nelle grammatiche context-free non viene sollevato il problema di *che tipo* di funzione è presente all'interno di un'espressione. Nelle grammatiche con contesto si può verificare che questa funzione sia stata definita, volendo anche 1000 righe prima, con il corretto tipo di ritorno.

Sebbene i linguaggi di programmazione abbiano caratteristiche sintattiche descrivibili in modo completo da grammatiche context-dependent, quest'ultime non si usano per non complicare la definizione sintattica del linguaggio. Questo compito viene delegato all'analizzatore *semantico*, che verifica esistenza della funzione, tipo di ritorno e numero e tipo degli argomenti.

# Classificazione dei linguaggi di Chomsky
| Tipo | Grammatica | Automa | Linguaggio |
|-----|-------------|--------|------------|
|3|regolare|automa finito|regolare|
|2|context-free|automa e stack|libero|
|1|context-dependent|automa e RAM|dipendente dal contesto|
|0|ricorsiva|macchina di Turing|ricorsivamente enumerabile|

#Esercizio linguaggio delle parentesi bilanciate $E \rightarrow \epsilon | (E)E$ attraverso un ragionamento induttivo

# Categorie sintattiche
Ogni categoria sintattica definisce un sottolinguaggio, con le sue regole e a sua volta la sua sintassi. È un linguaggio all'interno del linguaggio.

Convenzioni:
- caratteri in virgolette
- caratteri non terminali in corsivo

L'ambiguità viene tollerata dai linguaggi, non risolta a questo livello

#x016 
#x017 Ci sono due derivazioni ambigue (sostanzialmente differenti)

I parser usano algoritmi ricorsivi con backtracking (se sbaglio torno indietro alle produzioni in cui avevo più scelte disponibili).

I linguaggi regolari possono essere descritti da grammatiche libere (overpower) #x018

# Grammatiche equivalenti
Due grammatiche equivalenti che generano espressioni aritmetiche:
$$E \to E+E | E*E$$
$$E \to (E) | number$$

Oppure una grammatica equivalente che forza la precedenza degli operatori:
$$E \to E+E$$

#Completa 
# Parse tree
O alberi di derivazione
È un albero radicato ed etichettato:
- i nodi interni sono etichettati con simboli non terminali
- le foglie sono etichettate con simboli terminali o $\epsilon$
- la relazione padre-figlio è dettata dalle produzioni (nel nodo la parte destra, nelle foglie la parte sinistra)
- nella radice si trova l'assioma

Un albero sintattico descrive una derivazione? sì e no. Ad un parse tree possono corrispondere più derivazioni, ma questo non determinismo è marginale rispetto alla struttura dell'albero.

## Derivazioni canoniche
Derivazioni canoniche destre e sinistre.
- destra: sviluppa sempre il non terminale più a destra
- sinistra: sviluppa sempre il non terminale più a sinistra

Qual'è meglio? dipende dal tipo di parsing. Per il top down è meglio sinistre, per il bottom up è meglio derivazioni destre.

Fissata una derivazione canonica esiste un cammino unico sull'albero.

## Grammatiche ambigue
Si chiamano **ambigue** le grammatiche per cui anche se fissi una derivazione canonica per una stessa stringa possono essere generati parse tree differenti.

#x019

>Il parsing è il passaggio da una struttura lineare a una struttura bidimensionale

#Vedi if aperti (senza else) e chiusi. L'else viene fatto matchare con l'if più vicino.

Condizionale aperto e chiuso. Esiste una grammatica senza interpretazioni ambigue.
#Completa 

## Grammatiche cicliche
Se sono presenti derivazioni del tipo $A \Rightarrow A$
I parser usano algoritmi deterministici. Seguita una strada di produzioni seguiranno sempre quella. Si possono piantare in cicli.

I parser usano un *lookahead* -> sguardo in avanti di N simboli

## Grammatiche con prefissi comuni
Con due o più produzioni relative allo stesso nonterminale
$$S \to A\alpha_1$$ $$S \to A\alpha_2$$

Algo che risolve il problema:
- si numerano i terminali
- non può avvenire che una produzione abbia a destra un non terminale di grado più basso del non terminale di sinistra. Devono essere sempre piccolo -> grande

Viene snaturata leggermente la grammatica

#x020 Verrà mai fuori la derivazione sbagliata? No, viene forzato l'uso delle parentesi
#x021 usa sempre l'ordine degli operatori corretto

#Ricorda di sommare sempre dai valori più piccoli a quelli più grandi

Associatività:
- per, più e diviso a sinistra
- sottrazione ed esponenziale associativi a destra

Grammatica che parsa correttamente tutte le espressioni aritmetiche:
#completa con slide *Precedenza degli operatori*

#Esercizio grammatica libera per il linguaggio binario contenente tutte e sole le stringhe con più zeri che uni. O in generale tutte e sole le stringhe con un diverso numero di 1 e 0.