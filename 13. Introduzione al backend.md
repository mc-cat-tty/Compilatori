# Materiale

- **Embedded System Design**, Marwedel, Springer, 2018
- **Computers as Components 4th ed**, M. Wolf, Morgan Kaufmann O'Reilly, 2016
- **High Performance Embedded Computing 2nd ed**, M. Wolf, MK, 2014
- **Introduction to Embedded Systems, A Cyber-Physical Approach**, Lee and Seshia, MIT, 2007
- **Compiler Construction**, Denker, University of Bern
- **Compiler**, Hua, University of Science and Technology of China
- **Compiler Optimization**, Pekhimenko, University of Toronto

#Vedi Alberto Sangiovanni-Vincentelli
# Obiettivi
- Conoscenza elle ottimizzazioni nei compilatori
- Esperienza nell'implementazione di ottimizzazione con LLVM

>Passi di ottimizzazione

LLVM17 (?)

# Esame
Prova orale + assignments laboratoriali

# Motivazioni
Perchè studiare gli internals dei compilatori? computer pervasivi e onnipresenti

```
Programmi
|
| Compilatori
v
ISA
| Interfaccia hw/sw
CPU
Reti logiche
Silicon integration
```

#Vedi register file

>**Toolchain di compilazione** è una serie di strumenti che, usati in catena, producono un oggetto eseguibile a partire da codice sorgente:

```
															Startup code
																|
																v
source code -compiler-> asm files -assembler-> obj files -> ld (linker) -> exec
																^
																|
														linker scripts (.ld)
														& library objects
```

#Nota che l'entry point del codice non è eseguito immediatamente, all'avvio del programma. Viene prima eseguito 

Prima del *compilatore* si presenta spesso il *precompilatore*, che integra funzioni di macro expansion.
#Vedi `m4` macro processor

## Metriche di ottimizzazione
Si può ottimizzare per diversi scopi: performance, memoria, uso di batteria.
Spesso si richiede un tradeoff tra queste metriche. Eg: **efficienza energetica** (performance rispetto all'energia utilizzata)

Si può ad, esempio, ottimizzare per sfruttare al meglio la memoria cache.

## Evoluzione dei calcolatori
Fino agli anni 2000 si cerca di migliorare le performance **aumentando la frequenza di clock**.

>All'incirca ogni decennio nasce una nuova classe di calcolatori che costa meno della precedente, ma si basa su una nuova piattaforma, tipo di rete e interfaccia
>**Gordon Bell**

>Ogni anno e mezzo (18 mesi) si dimezza la dimensione del transistor
>**Gordon Moore**

Andando avanti con questo trend (**aumento della densità di potenza**) si è raggiunto il **power wall**.

La soluzione (temporanea) al problema diventa **aggiungere più core**. Si prova addirittura ad abbassare la frequenza per abbassare la densità di potenza. \[2004\]
Cosa succede alla performance delle applicazioni:
- **Performance sequenziali** si inflettono come il consumo
- **Performance parallele** crescono, ma il codice legacy è stato progettato per funzionare in single core

## Compilatori multicore
Due soluzioni:
- da un lato la transizione si affronta con compilatori **auto-parallelizzanti**
- dall'altro una *pletora* di nuovi **parallel programming models** che espongono al programmatore una semplice interfaccia. Eg. CUDA per GPU e OpenMP per CPU

#Rivedi CUDA e OpenMP, Amdahl's law, data vs instruction level parallelism

OpenMP nasce in ambiente di super-calcolatori, dove il calcolo parallelo esisteva prima dell'avvento dei sistemi multicore. Nei super-calcolatori si effettua calcolo scientifico, campo in cui il **Fortran** regna sovrano. Quindi OpenMP nasce in Fortran.

#Vedi `pthreads`, embarrassingly parallel application

OpenMP usa meta-istruzioni tipo `#pragma omp parallel for`. Layouting del for in una funzione separata (problematica impacchettamento e scambio dati). Interfacciamento con il runtime environment; chiamata, ad esempio, a `pthread_create`.

## Architetture asimmetriche
Si introduce l'introduzione di core eterogenei. Eg: GPU, FPGA, TPU, asymmetric cores on the same SoM/SoC

Questo porta ad una suddivisione dei compiti.

Ulteriormente, le GPU diventano negli anni GPGPU - General Purpose GPU. Ovvero sottosistemi in grado di "fare bene" anche calcolo parallelo che non sia legato alla grafica.

# Misura delle performance
$$
perf = \frac{1}{exec\_time}
$$
$$
E_t = \frac{instructions \times CPI}{freq}
$$

$E_t$ execution time
$CPI$ cycles/clocks per instruction

Controllo dimensionale: $E_t = \frac{istr \times \frac{cycle}{istr}}{\frac{}{}}$

Cosa può fare un programmatore per migliorare il tempo di esecuzione:
- freq deriva dalla progettazione su silicio
- CPI non posso cambiare il CPI di una istruzione, ma posso sceglierle opportunamente. Eg. le istr aritmetico-logiche richiedono un solo ciclo, ad esclusione di mul e div. Il branching deve mettere in conto la misprediction, il loading la mancata hit in cache

Esempio: `mul s0, s3, 8` (3 cicli) posso trasformarlo in `slli s0, s3, 1`
Questa trasformazione non abbassa il numero di istruzioni, ma il CPI medio.

# Esempi di ottimizzazioni
Distingui **compile-time optimisations** ed **executions/runtime optimisation**

## Algebraic Simplifications
```
Semplificazioni algebriche -(-i) => i
Cortocircuito logico (a > 100) or true => true
```

## Constant Folding
```
int a = 1 + 3 => 4
100 < 0 => false
```

## Strength Reduction
Rimpiazza operazioni costose con operazioni più semplici.

```
x * 2 => x + x
x * 17 => (x << 4) + x
```

Prima di trasformare un'operazione devo valutare sempre che:
- la trasformazione sia valida: equivalente di quella originale
- la trasformazione porti un beneficio: altrimenti lascio perdere

Posso rimpiazzare la logica di accesso sequenziale da un semplice accesso diretto ad un accesso con accumulatore (nota che è una **doppia SR**):
```
for (int i=0; i<100; i++) { a[i] = i*100; }
|
V
t = 0
for (; t < 1e4; t += 100) {
	*a = t;
	a += 4;
}
```

Supposto che a sia un array di interi (dimensione 4 byte), per accedere all'elemento i-esimo:
- a0 array base address
- s0 offset register
- s0 * 4 + a0 => (s0 << 2) + a0