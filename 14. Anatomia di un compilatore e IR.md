# Overview di un compilatore
Un compilatore deve svolgere almeno due compiti:
- **analisi** del codice da ottimizzare
- **sintesi** del codice ottimizzato

Si usa il termine *passi*: passi di sintesi o di ottimizzazione.

Una vista a blocchi di un compilatore:
```
-Source code-> |Frontend compiler| -IR-> |Backend compiler| -target code->
```
La IR - Intermediate Representation - è una rappresentazione.

Un esploso sulla freccia centrale (**middle end**) porta a:
```
-IR-> |Optimizer| -IR->
		|  ^
		|__|
```

## Nel progetto LLVM
La struttura di LLVM, a differenza del monolitico gcc, è modulare:
 - `clang` front-end
 - `opt` middle-end optimizer
 - `llc` back-end fornito di machine-description

## Flag di ottimizzazione
L'utilizzo generico di un compilatore prevede l'invocazione di un driver:
- `-g` abilita i simboli di debug e inibisce le ottimizzazioni; esistono diversi livelli di debug
- `-O` abilita l'ottimizzazione; esistono diversi livello
	- `-O0` nessuna ottimmizzazione
	- `-O1` ottimizzazioni che non impiegano molto tempo.
	- `-O2` default. L'IR entra in un feedback loop di ottimizzazione. Più aggressivo.
	- `-O3` loop unrolling, inlining aggressivo
	- `-Os` ottimizza in termini di dimensioni (size): sia del processo di ottimizzazione che del programma che si sta compilando

Perchè non ottimizzare sempre al massimo? dipende dal tradeoff tempo ottimizzazione - interesse nell'ottimizzazione (quanto vogliamo che l'ottimizzazione sia **aggressiva**).

Cos'è l'**inlining**? la chiamata a funzione porta ad un overhead. Tutte le invocazioni possono essere sostituite direttamente con il codice della funzione. Ma aumenta la dimensione del compilato, con `-Os` viene applicato con parsimonia.

## Motivazione IR
1. modularità secondo i principi cardine dell'ingegneria del software
2. **retargeting** verso nuova ISA: isolamento back e front end
3. Supporto a nuovi linguaggi di programmazione

# Ingredienti ottimizzazione
Le istruzioni **statiche** della IR vengono spesso astratte sotto forma di modelli matematici (grafi, matrici, programmazione lineare, relazioni). I modelli portano a soluzioni riutilizzabili per problemi comuni a più programmi, che cambiano ancora una volta rappresentazione trasformandosi in codice generato.

>**Pattern di accesso**

#Vedi rappresentazioni poliedrali

# IR
## Proprietà
Nel ciclo di ottimizzazione possiamo individuare:
- Passo di analisi: consuma IR
- Passo di trasformazione: produce IR

Quali principi segue la progettazione di una IR?
- facilità di generazione
- facilità di manipolazione: evidenziazione dipendenze e simili
- costo di manipolazione: l'analisi di codice IR satura la RAM? compilatore poco performante
- livello di astrazione: influenza quanto è facile applicare ottimizzazioni
- livello di dettaglio

## Rappresentazioni
- AST - Abstract Syntax Trees
- DAG - Directed Acyclic Graphs
- 3AC - 3-Address Code
- SSA - Static Single Assignments
- CFG - Control Flow Graphs
- CG - Call Graphs
- PDG - Program Dependencies Graphs

#Nota che non esistono solo compilatori da sorgente a macchina, ma anche source to source translation

## Categorie di IR
- **grafiche**. Eg. DAG, AST. Orientate ai grafi e usate in SST; tipicamente voluminose
- **lineari**. Eg. 3AC. Sintassi simile all'assembly per architettura astratta; facile da riarrangiare; numero infinito di registri.
- **ibride**. Eg. CFG. Combinazione di forme grafiche e lineari.

Una **rappresentazione** può essere **concreta** o **astratta**.

### AST
>Albero in cui i nodi rappresentano diverse parti del programma. Sulle foglie si trovano gli operandi.

Offre una descrizione ricca per un linguaggio ma non è utile ai fini dell'ottimizzazione, sarebbe impratico.

### DAG
>Nel mondo dei compilatori sono tipicamente contrazioni degli AST che evitano la ripetizione di operandi sulle foglie.

Potrebbe essere vista come una prima forma di CSE - Common Subexpression Elimination -, ma non include la nozione di "tempo". Devo avere la garanzia che nel mentre non cambino gli operandi (non siano aggiornati).

I DAG danno vita a rappresentazioni ambigue.

### 3AC
Negli ottimizzatori si usano rappresentazioni lineari, spesso le 3AC.

>Rappresentazione lineare in cui le istruzioni hanno la forma `x = y op z`. Un unico operatore e al massimo due operatori sorgente.

Si è costretti ad utilizzare registri virtuali (infiniti), ovvero temporanei, che permettano di spezzare qualsiasi istruzione del programma sorgente.

- assigments:
	- binario `x = y op z`
	- unario `x = op y`
	- diretta `x = y`
	- deref `x = y[1]`
- unconditional branch: `goto L`
- conditional branch: `if x relational_op y goto L`
- procedure calls (chiamata a funzione): `param x; param y; call f`
- address and pointer assignments: `x = &y; *y = z`

#Nota che la memoria è puramente virtuale a questo livello

#### Rappresentazione
La rappresentazione della 3AC può essere implementata mediante quadruple o triple:
- quadruple con al max 1 operando dst e al max 2 operandi src
- triple con istr e almeno 1 operando, al massimo 2

Es:
```
(1) load t1, y
(2) load t2, x
(3) mul t3, t1, t2
```

vs

```
(1) load y
(2) load x
(3) mul (1), (2)
```

La seconda rappresentazione è più compatta: l'operando destinazione diventa un tutt'uno con l'operazione stessa (associazione risultato-riga istruzione)

# Analisi rappresentazione-ottimizzazioni
## 3AC for CP
La forma 3AC rende palesi le modifiche di una variabile? condizione necessaria per attuare correttamente la Constant Propagation.

>Reminder: la CP sostituisce usi futuri di una variabile con costante ad essa assegnata, se non modificata nel frattempo.

Nella forma 3AC non è direttamente applicabile la CP senza un'analisi dell'evoluzione temporale della variabile.

Si può utilizzare un algoritmo chiamato #Completa .

Si preferisce però la SSA. Ogni variabile può essere assegnata una sola volta. Per ogni istruzione possiamo ricavare una lista di puntatori dove il risultato viene utilizzato.
**Semplifica la manipolazione della IR**.

Con la forma SSA diventa immediato propagare la costante, in quanto si evitano le ridefinizioni della stessa variabile:
```
b1 = 2
a = b1 + 10
c = b1 * 7
b2 = 20
d = b2 + 20
```

## Control Flow Graph
>Non esiste una forma perfetta di IR per tutti gli scopi.

Per il momento non abbiamo tenuto conto del controllo di flusso (abbiamo visto solo sequenze lineari di operazioni); è necessaria un'analisi di come varia il PC nell'esecuzione del programma. Eg. **CFG + 3AC**

Un CFG modella il trasferimento (flusso) del controllo con blocchi. Costituiscono i nodi del grafo. Gli archi rappresentano il flusso di controllo.

>**Basic Block** o blocco di programma: sequenza lineare di istruzioni terminata da un'istruzione di trasferimento del controllo (istruzione che alteri il PC).

Ripasso: il valore del PC è pilotato da un multiplexer con due ingressi. Il primo è un sommatore con un feedback sul PC attuale e 4 (lunghezza di una istruzione su una macchina a 32 bit). Il secondo è un sommatore con un feedback sul PC attuale e un immediato llsi di 1 (per salvare 1 bit in meno).

### Proprietà del basic block
- ogni basic block ha un **single entry point**: solo la prima istruzione può essere raggiunta dall'esterno (non si può entrare nel blocco "dal mezzo")
- ogni basic block ha un **single exit point**: tutte le istruzioni del blocco vengono eseguite se si entra nel blocco

#Attenzione la prima proprietà è ben diversa dal dire che **il BB può essere raggiunto da un solo arco entrante**

Riassunta in **SESE** - Single Entry Single Exit

Questa regola permette di individuare **regions** con la stessa proprietà.

- Un arco connette due nodi Bi->Bj se e solo se Bj può eseguire dopo Bi:
	- la prima istruzione di Bj è il target dell'istruzione di salto di Bi
	- la prima istruzione di Bj è sequenziale all'ultima Bi. Si parla di **arco fallthrough**

Normalizzando il concetto di **arco fallthrough** potrei creare blocchi minimali da 1 istruzione senza violare nessuna proprietà.

>Un CFG si dice **normalizzato** se ha **blocchi massimali**, senza violare nessuna regola di costruzione.

### Algoritmo di costruzione
1. identificare il leader di ogni basic block
2. identificare l'ultima istruzione di ogni BB: l'istruzione che precede un leader
3. identificare gli archi del BB: fallthrough o branch (chiamati *true* e *false branch*)

#Vedi esempio slide 37

## Dependency Graph
I nodi di DG sono istruzioni che **usano altri valori** e ne definiscono altri. Ogni istruzione dipenderà da al più altre due istruzioni.

Utile per evitare data hazard e fare instruction rescheduling, nel caso in cui l'architettura non lo supportasse.

Il DCG può essere costruito a diversi livelli:
- instruction level: guardando quali registri asm sono utilizzati
- data level: ad esempio per parallelizzare un loop, se possibile. **DDG - Data Dependency Graph**

Ma non sempre è possibile parallelizare:
```
for (i=1; i<3; i++) {
	A[i] = A[-1];
}
```

Ho una dipendenza di dato tra diverse iterazioni del loop.

Queste analisi si applicano, per esempio, a loop nests. Un **poliedro** potrebbe ad esempi modellare un triplo loop innestato.

In LLVM esiste un framework per fare DDG, chiamato `POLLY`, per fare polyhedra analysis. In GCC esiste `GRAPHITE`.

## Call Graph
Mostra l'insieme delle potenziali chiamate tra le funzioni; è una rappresentazione gerarchica.

Ricombinando gli elementi: CG ad alto livello. Per ogni funzione rappresentazione CFG + 3AC-SSA

Cosa succede se il progetto è multi-file? Cerco di fare la **WPA - Whole Program Analysis**. Si applica il framework **LTO - Link-Time Optimization** a tempo di linking.
